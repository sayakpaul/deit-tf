{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aea44c48",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc128cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from vit.deit_models import ViTDistilled\n",
    "from vit.model_configs import base_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bfb385",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb7e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"deit_distilled_tiny_patch16_224\"\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 20\n",
    "BASE_LR = 0.0005\n",
    "WEIGHT_DECAY = 0.0001\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "NB_CLASSES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c283a3",
   "metadata": {},
   "source": [
    "## Initialize model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64a3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "deit_tiny_config = base_config.get_config(drop_path_rate=0.1, model_name=MODEL_TYPE)\n",
    "with deit_tiny_config.unlocked():\n",
    "    deit_tiny_config.num_classes = NB_CLASSES\n",
    "\n",
    "deit_tiny_config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca78ed",
   "metadata": {},
   "source": [
    "## Data preprocessing and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751dd339",
   "metadata": {},
   "outputs": [],
   "source": [
    "SZ = deit_tiny_config.image_size\n",
    "\n",
    "\n",
    "def preprocess_dataset(is_training=True):\n",
    "    def _pp(image, label):\n",
    "        if is_training:\n",
    "            # Resize to a bigger spatial resolution and take the random\n",
    "            # crops.\n",
    "            image = tf.image.resize(image, (SZ + 20, SZ + 20))\n",
    "            image = tf.image.random_crop(image, (SZ, SZ, 3))\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "        else:\n",
    "            image = tf.image.resize(image, (SZ, SZ))\n",
    "        label = tf.one_hot(label, depth=NB_CLASSES)\n",
    "        return image, label\n",
    "\n",
    "    return _pp\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset, is_training=True):\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 10)\n",
    "    dataset = dataset.map(preprocess_dataset(is_training), num_parallel_calls=AUTO)\n",
    "    return dataset.batch(BATCH_SIZE).prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57489d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = tfds.load(\n",
    "    \"tf_flowers\", split=[\"train[:90%]\", \"train[90%:]\"], as_supervised=True\n",
    ")\n",
    "num_train = train_dataset.cardinality()\n",
    "num_val = val_dataset.cardinality()\n",
    "print(f\"Number of training examples: {num_train}\")\n",
    "print(f\"Number of validation examples: {num_val}\")\n",
    "\n",
    "train_dataset = prepare_dataset(train_dataset, is_training=True)\n",
    "val_dataset = prepare_dataset(val_dataset, is_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca34d0ef",
   "metadata": {},
   "source": [
    "## Initialize student and teacher models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a7d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "deit_tiny = ViTDistilled(deit_tiny_config)\n",
    "\n",
    "resolution = deit_tiny_config.image_size\n",
    "dummy_inputs = tf.ones((2, resolution, resolution, 3))\n",
    "_ = deit_tiny(dummy_inputs)\n",
    "print(f\"Number of parameters (millions): {deit_tiny.count_params() / 1e6}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55968397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 98.37% on the validation set.\n",
    "# To know how this was trained refer to `./bit-teacher.ipynb`.\n",
    "bit_teacher_flowers = keras.models.load_model(\"bit_teacher_flowers\")\n",
    "print(f\"Number of parameters (millions): {bit_teacher_flowers.count_params() / 1e6}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f5cf1a",
   "metadata": {},
   "source": [
    "Here we can see that the teacher model has got orders of magnitude more parameters than the student model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31903aeb",
   "metadata": {},
   "source": [
    "## Wrap the training logic of DeiT\n",
    "\n",
    "**Note** that here we are just following the core principles of the distillation process laid out in the [original paper](https://arxiv.org/abs/2012.12877). The authors use more data augmentation and regularization which have been purposefully discarded to keep the workflow simple to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50352e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeiT(keras.Model):\n",
    "    # Reference:\n",
    "    # https://keras.io/examples/vision/knowledge_distillation/\n",
    "    def __init__(self, student, teacher, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "    ):\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data.\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = tf.nn.softmax(self.teacher(x, training=False), -1)\n",
    "        teacher_predictions = tf.argmax(teacher_predictions, -1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student.\n",
    "            cls_predictions, dist_predictions, _ = self.student(\n",
    "                x / 255.0, training=True\n",
    "            )\n",
    "\n",
    "            # Compute losses.\n",
    "            student_loss = self.student_loss_fn(y, cls_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                teacher_predictions, dist_predictions\n",
    "            )\n",
    "            loss = (student_loss + distillation_loss) / 2\n",
    "\n",
    "        # Compute gradients.\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights.\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        student_predictions = (cls_predictions + dist_predictions) / 2\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance.\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data.\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions.\n",
    "        y_prediction, _ = self.student(x / 255.0, training=False)\n",
    "\n",
    "        # Calculate the loss.\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance.\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.student(inputs / 255.0, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfc9b23",
   "metadata": {},
   "source": [
    "## Distill the teacher model into the student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64652180",
   "metadata": {},
   "outputs": [],
   "source": [
    "deit_distiller = DeiT(student=deit_tiny, teacher=bit_teacher_flowers)\n",
    "\n",
    "lr_scaled = (BASE_LR / 512) * BATCH_SIZE\n",
    "deit_distiller.compile(\n",
    "    optimizer=tfa.optimizers.AdamW(weight_decay=WEIGHT_DECAY, learning_rate=lr_scaled),\n",
    "    metrics=[\"accuracy\"],\n",
    "    student_loss_fn=keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, label_smoothing=0.1\n",
    "    ),\n",
    "    distillation_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "_ = deit_distiller.fit(train_dataset, validation_data=val_dataset, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13614ef9",
   "metadata": {},
   "source": [
    "The model should give about 68.5% - 69.5% accuracy on the validation set. The results may slightly vary depending on the hardware used. \n",
    "\n",
    "If the same student model was trained from scratch (i.e., without distillation) it would give 65% - 66% accuracy on the validation set. To train such a model adapt the following code:\n",
    "\n",
    "```py\n",
    "inputs = keras.Input((SZ, SZ, 3))\n",
    "x = keras.layers.Rescaling(scale=1./255)(inputs)\n",
    "outputs, _ = deit_tiny(x) # Second output in the tuple is a dictionary containing attention scores.\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(...)\n",
    "model.fit(...)\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
